---
title: Andrew Ng 머신러닝 강의 노트 Lecture 2
date: 2020-02-27 11:34:00 +0900
categories: [Blogging, Programming]
tags: [ai, ml]
toc: true
sitemap:
  changefreq: daily
  priority: 1.0
seo:
  date_modified: 2020-03-05 13:56:20 +0900
---

***

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.1 - Linear Regression With One Variable**</span>

&nbsp;&nbsp;&nbsp;&nbsp;세상엔 다양한 집들이 있을 것이다. 이 집들을 x축은 집의 면적, y축은 집의 가격에 해당하는 직교 좌표 위에 데이터로 나타난다고 가정해보자. 대략적으로 양의 상관관계가 있는 그래프가 나올 것이다. 여기서 집의 면적과 가격 두 값에 대한 학습을 한다고 가정하자.<br>

&nbsp;&nbsp;&nbsp;&nbsp;사전에 수집한 집값 정보라는 정답이 존재하므로 supervised learning에 해당하며, 추정하고자 하는 값이 실수값이므로 regression problem이다. 만약 일차함수에 비슷한 직선 그래프가 나온다면 linear regression problem이 된다.

<center>m: # of training examples</center>
<center>x’s = “input” variable / features</center>
<center>y’s = “output” variable / “target” variable</center>
로 정의되었을 때,

<center>(x, y) - one training example<br></center>
<center>(x<sup>(i)</sup>, y<sup>(i)</sup>) - i<sub>th</sub> training example</center>
와 같은 방식으로 접근할 수 있다.

&nbsp;&nbsp;&nbsp;&nbsp;일단 기본 구조는 training set에 대해, 알고리즘을 도출하여 알맞는 가설(hypothesis)을 만들고 검증하는 것이다. 여기서 x값은 집의 면적, y값은 집의 가격이 될 것이다. 가설은 x값으로부터 y값을 도출하는 특정 식이 될 것이다.

&nbsp;&nbsp;&nbsp;&nbsp;그렇다면 우리는 가설을 어떻게 세우는가? 가설은 어떤 형태의 함수로도 가능하지만, 일단 선형함수가 가장 많이 쓰인다.<br>
<center>$h_{θ}(x) = θ_{0}+θ_{1}x$ 와 같이 말이다.</center><br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.2 - Linear Regression With One Variable | Cost Function**</span>

&nbsp;&nbsp;&nbsp;&nbsp;가설을 $$h_{θ}(x) = θ_{0}+θ_{1}x$$와 같은 선형함수로 세웠다고 치자. 그러면 우리는 이 가설이 잘 맞아떨어지는 지 어떻게 확인할 수 있을까?
일단 여기서 $$θ_{1}$$, $$θ_{2}$$이 두 변수(parameter)로써 존재하므로, 이들에 대한 타당성이 증명되어야 할 것이다. 그리고 이를 평가할 수 있는 수치가 바로 cost function이다.



&nbsp;&nbsp;&nbsp;&nbsp;핵심 아이디어는 $$θ_{i}$$를 잘 골라서 $$h_{θ}(x)$$이 $$y$$에 가깝게 하는 것이다.
<center>$$(1)\;(h_{θ}(x)-y)^2$$  $⋯(1)$ 한 sample에서 sample값($y$)과 가설값($x$)의 차이를 수치화한다.</center>
<center>$$(2)\;\sum\limits_{i=1}^{m}{(h_{θ}(x^{(i)})-y^{(i)})^2}$$ $⋯(2)$ 이를 sample 수 만큼 더한 뒤</center>
<center>$$(3)\;cost function = (\cfrac{1}{2m})\sum\limits_{i=1}^{m}{(h_{θ}(x^{(i)})-y^{(i)})^2}$$ $⋯(3)$ sample 수 만큼 나누어서 평균을 구한다. 2를 더 나눈 것은 수학적 편의를 위한 것이라고 한다.</center><br>
&nbsp;&nbsp;&nbsp;&nbsp;여기서 cost function은 $$J(θ_{0}+θ_{1})$$로도 표현된다.<br>
<br>

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.3 - Linear Regression With One Variable | Cost Function Intuition**</span>


&nbsp;&nbsp;&nbsp;&nbsp;직관적인 접근을 위해 $θ_{0}$를 $0$으로 두면, $h_{θ}(x) = θ_{1}x$ 이다. 목표도 $J(θ_{1})$를 최소화하는 것이 된다. 가로축을 $θ_{1}$, 세로축을 $J(θ_{1})$으로 한 그래프를 그린다면, 아래로 볼록한 그래프가 그려지게 되는데,
여기서 $J(θ_{1})$값이 최소가 되는 지점이 바로 Cost function이 가장 최소인, 즉 가장 잘 들어맞는 가설 지점이 되는 것이다.<br>

&nbsp;&nbsp;&nbsp;&nbsp;아래는 내가 직접 그린 그림으로, {(1,1),(2,2),(3,3)}의 training set가 있을 때의 조건이다.<br>
![tansex1](/images/posts/2020-02-27-tensorflow1/tensex1.png){:width="60%"}{: .center}

&nbsp;&nbsp;&nbsp;&nbsp;<span style="line-height:160%; font-size: 19px; color:#519d9e;">**Lecture 2.3 관련 실습**</span>

&nbsp;&nbsp;&nbsp;&nbsp;위 그림을 만들 수 있는 코딩을 아래 포스트에 정리해두었다.<br>
&nbsp;&nbsp;&nbsp;&nbsp;[(바로가기)Tensorflow 실습 1](https://now-man.github.io/posts/tensorflow1/)

## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.4 - Linear Regression With One Variable | Cost Function Intuition #2**</span>

&nbsp;&nbsp;&nbsp;&nbsp;위에서는 parameter를 하나로 고정했지만, 일반적인 선형함수($h_{θ}(x) = θ_{0}+θ_{1}x$)에서는 parameter가 두 개다. 이 두 parameter에 대해 $J(θ_{0}+θ_{1})$ 그래프를 그리면 다음과 같이 3차원의 그래프가 나온다. 이번에도 $J(θ_{0}+θ_{1})$ 값이 가장 낮은 지점이 가장 일치하는 가설임은 동일하다.<br>
![tumor2](/images/posts/2020-02-27-ai2/cost3d.png){:width="60%"}{: .center}

&nbsp;&nbsp;&nbsp;&nbsp;$θ_{0}$,$θ_{1}$을 축으로 삼고,$J(θ_{0}+θ_{1})$값이 같은 선을 이어주어 2차원 그래프를 그리면 등고선 모양이 된다. 여기서도 등고선의 가장 중앙이 가장 일치하는 가설이다.<br>
![tumor2](/images/posts/2020-02-27-ai2/cost2d.png){:width="60%"}{: .center}


## <span style="line-height:160%; color:#7f71ad; font-family: 'Noto Serif KR';">**· Lecture 2.5 - Linear Regression With One Variable | Gradient descent**</span>

&nbsp;&nbsp;&nbsp;&nbsp;이제 cost function에 대해 대충 알았으니, 이를 자동으로 구해주는 알고리즘이 필요하다.
핵심 아이디어는 특정 $θ_{0}$, $θ_{1}$을 찝어서, 이를 조금조금씩 변화시키며 $J(θ_{0}+θ_{1})$의 최소값을 찾아내는 것이다.
$θ_{i}$는 뭘로 시작해도 괜찮지만 $0$으로 시작하는 게 일반적이라고 한다.



<center>$\theta_{j}:=\theta_{j}-\alpha\cfrac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad(for\;j = 0\;and\; j = 1)$</center>

&nbsp;&nbsp;&nbsp;&nbsp; 위의 식이 바로 gradient descent algorithm이다.
여기서 $:=$는 assignment라고 한다. $a:=b$라고 함은 $b$의 값을 $a$에 적용하겠다는 것인데, 예를 들면 $a:=a+1$일 때는 a의 값이
1씩 늘어난다고 보면 된다. $\alpha$는 learning rate인데, $\alpha$가 작을 수록 더 세밀하게 검사한다고 보면 된다.
$\frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$은 cost function을 $\theta_j$에 대해 편미분한 값으로써 기울기라 보면 된다.

&nbsp;&nbsp;&nbsp;&nbsp;


$\cfrac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$<br>
$=\cfrac{\partial}{\partial\theta_{j}}[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  h_\theta(x^{(i)})-y^{(i)}  \right)^2]$<br>
$=\cfrac{\partial}{\partial\theta_{j}}[\cfrac{1}{2m}\sum\limits_{i=1}^{m}\left(  \theta_0 + \theta_{1}x^{(i)})-y^{(i)}  \right)^2]$

## **참고**
유튜브: <https://www.youtube.com/channel/UC5zx8Owijmv-bbhAK6Z9apg><br>
위키독스: <https://wikidocs.net/4213>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script>
<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>