---
title: Andrew Ng 머신러닝 강의 노트 Lecture 2
date: 2020-02-27 11:34:00 +0900
categories: [Blogging, Programming]
tags: [ai, ml]
toc: true
sitemap:
  changefreq: daily
  priority: 1.0
seo:
  date_modified: 2020-02-27 11:35:00 +0900
---

Andrew Ng 교수님의 명강의를 노트해봤다. (아직 미완성글)

***

## **Lecture 2.1 - Linear Regression With One Variable**

집의 size와 price 간 학습을 한다고 가정하자.<br>
사전에 수집한 집값 정보라는 정답이 존재하므로 supervised learning에 해당하며, 추정하고자 하는 값이 실수값이므로 regression problem이다.

m: # of training examples<br>
x’s = “input” variable / features<br>
y’s = “output” variable / “target” variable<br>

(x, y) - one training example<br>
(x(i), y(i)) - ith training example<br>

[training set] -> [learning algorithm] -> [hypothesis]<br>
(hypothesis maps from x’s to y’s)<br>

How do we represent h?<br>
Hypothesis can be taken in any form of function, but the following linear functions are frequently used.<br>
 > hθ(x) = θ0+θ1x
 
(linear regression with one variable) = univariate linear regression<br>


## **Lecture 2.2 - Linear Regression With One Variable | Cost Function**

θi’s -> parameters<br>
So how do we come up with the θi that corresponds to a good fit to the data?<br>
Idea: θi를 잘 골라서 hθ(x)이 y에 가깝게 하는 것 -> 평가 대상 필요<br>
(hθ(x)-y)2 > 한 sample에서 차이를 나타내는 법<br>
cost function: J(θ0+θ1) <br>

## **Lecture 2.3 - Linear Regression With One Variable | Cost Function Intuition**

직관적인 접근을 위해 θ0를 0으로 두면, hθ(x) = θ1x 이다. 목표도 J(θ1)를 최소화하는 것.<br>

## **참고**
유튜브: <a>https://www.youtube.com/channel/UC5zx8Owijmv-bbhAK6Z9apg<a>
위키독스: <a>https://wikidocs.net/4213<a>